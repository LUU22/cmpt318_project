{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "import datetime\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(label):\n",
    "    r = np.zeros(total_labels, dtype=int)\n",
    "    r[label] = 1\n",
    "    return r.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=False):\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    # 每个epoch的num_batch\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    print(\"num_batches_per_epoch:\",num_batches_per_epoch)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/weather_data.csv', encoding = \"ISO-8859-1\")\n",
    "index = data.columns.values\n",
    "data_length = data.shape[0]\n",
    "\n",
    "weather_type = set(data['Weather'])\n",
    "weather_dict = dict(zip(weather_type,list(range(len(weather_type)))))\n",
    "\n",
    "total_labels = len(weather_type)\n",
    "\n",
    "\n",
    "data['Weather'].replace(pd.Series(weather_dict), inplace=True)\n",
    "\n",
    "#data['Weather'] = data['Weather'].update(pd.Series(weather_dict))\n",
    "data['Weather'] = data['Weather'].apply(one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Clear                        19\n",
       "Cloudy                       14\n",
       "Drizzle,Fog                  12\n",
       "Fog                          18\n",
       "Freezing Fog                  3\n",
       "Mainly Clear                 17\n",
       "Moderate Rain                15\n",
       "Moderate Rain,Fog             7\n",
       "Moderate Snow,Fog            16\n",
       "Mostly Cloudy                 8\n",
       "Rain                         13\n",
       "Rain Showers                  0\n",
       "Rain Showers,Snow Showers    11\n",
       "Rain,Drizzle,Fog              6\n",
       "Rain,Fog                      1\n",
       "Rain,Snow                     9\n",
       "Rain,Snow,Fog                 2\n",
       "Snow                         10\n",
       "Snow Showers                  5\n",
       "Snow,Fog                      4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(weather_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(pd.Series(weather_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_length = int(data_length * 0.8)\n",
    "\n",
    "train_data = data[:train_data_length]\n",
    "test_data = data[train_data_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_x = index[:-1]\n",
    "index_y = index[-1]\n",
    "\n",
    "x_train = train_data[index_x].values\n",
    "y_train = np.asarray(train_data[index_y].values.tolist())\n",
    "\n",
    "x_test = test_data[index_x].values\n",
    "y_test =  np.asarray(test_data[index_y].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define 2 placeholders\n",
    "x = tf.placeholder(tf.float32, [None, x_train.shape[1]], name=\"input_x\")\n",
    "y = tf.placeholder(tf.float32, [None, y_train.shape[1]], name=\"input_y\")\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([x_train.shape[1],32], stddev=0.1), name='W1')\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[32]), name='b1')\n",
    "prediction1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([32,32], stddev=0.1), name='W2')\n",
    "b2 = tf.Variable(tf.constant(0.1, shape=[32]), name='b2')\n",
    "prediction2 = tf.nn.relu(tf.matmul(prediction1, W2) + b2)\n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([32,total_labels], stddev=0.1), name='W3')\n",
    "b3 = tf.Variable(tf.constant(0.1, shape=[total_labels]), name='b3')\n",
    "prediction = tf.nn.relu(tf.matmul(prediction1, W3) + b3)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.0002\n",
    "decay_steps = 5000\n",
    "decay_rate = 0.95\n",
    "    \n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, decay_steps=decay_steps, \n",
    "                                           decay_rate=decay_rate, staircase=True)\n",
    "    #optimizer = tf.GradientDescent(learning_rate)\n",
    "    #optimizer.minimize(...my loss..., global_step=global_step)\n",
    "\n",
    "\n",
    "add_global = global_step.assign_add(1)\n",
    "with tf.control_dependencies([add_global]):\n",
    "    #train_op = opt.minimise(loss)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction,1))\n",
    "\n",
    "#求准确率\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_list = [[],[],[], []] #[[iterater],[training_accuracy],[testing_accuracy], [loss]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batches_per_epoch: 527\n",
      "iterator 1000, testing accuracy 0.2280 , training accuracy 0.1000 , learning rate 0.000200\n",
      "iterator 2000, testing accuracy 0.1911 , training accuracy 0.1000 , learning rate 0.000200\n",
      "iterator 3000, testing accuracy 0.2249 , training accuracy 0.2000 , learning rate 0.000200\n",
      "iterator 4000, testing accuracy 0.2261 , training accuracy 0.3000 , learning rate 0.000200\n",
      "iterator 5000, testing accuracy 0.2363 , training accuracy 0.1500 , learning rate 0.000190\n",
      "iterator 6000, testing accuracy 0.2017 , training accuracy 0.2500 , learning rate 0.000190\n",
      "iterator 7000, testing accuracy 0.2169 , training accuracy 0.2000 , learning rate 0.000190\n",
      "iterator 8000, testing accuracy 0.2553 , training accuracy 0.2500 , learning rate 0.000190\n",
      "iterator 9000, testing accuracy 0.2268 , training accuracy 0.3500 , learning rate 0.000190\n",
      "iterator 10000, testing accuracy 0.1862 , training accuracy 0.1000 , learning rate 0.000180\n",
      "iterator 11000, testing accuracy 0.2451 , training accuracy 0.2500 , learning rate 0.000180\n",
      "iterator 12000, testing accuracy 0.2454 , training accuracy 0.2500 , learning rate 0.000180\n",
      "iterator 13000, testing accuracy 0.1983 , training accuracy 0.1000 , learning rate 0.000180\n",
      "iterator 14000, testing accuracy 0.2386 , training accuracy 0.3000 , learning rate 0.000180\n",
      "iterator 15000, testing accuracy 0.1934 , training accuracy 0.2000 , learning rate 0.000171\n",
      "iterator 16000, testing accuracy 0.2200 , training accuracy 0.2000 , learning rate 0.000171\n",
      "iterator 17000, testing accuracy 0.2401 , training accuracy 0.1000 , learning rate 0.000171\n",
      "iterator 18000, testing accuracy 0.2257 , training accuracy 0.2000 , learning rate 0.000171\n",
      "iterator 19000, testing accuracy 0.2397 , training accuracy 0.4500 , learning rate 0.000171\n",
      "iterator 20000, testing accuracy 0.2489 , training accuracy 0.1000 , learning rate 0.000163\n",
      "iterator 21000, testing accuracy 0.2515 , training accuracy 0.1500 , learning rate 0.000163\n",
      "iterator 22000, testing accuracy 0.2492 , training accuracy 0.1500 , learning rate 0.000163\n",
      "iterator 23000, testing accuracy 0.2595 , training accuracy 0.1000 , learning rate 0.000163\n",
      "iterator 24000, testing accuracy 0.2470 , training accuracy 0.2000 , learning rate 0.000163\n",
      "iterator 25000, testing accuracy 0.2401 , training accuracy 0.1500 , learning rate 0.000155\n",
      "iterator 26000, testing accuracy 0.2375 , training accuracy 0.2000 , learning rate 0.000155\n",
      "iterator 27000, testing accuracy 0.1903 , training accuracy 0.3500 , learning rate 0.000155\n",
      "iterator 28000, testing accuracy 0.1862 , training accuracy 0.2000 , learning rate 0.000155\n",
      "iterator 29000, testing accuracy 0.2268 , training accuracy 0.2000 , learning rate 0.000155\n",
      "iterator 30000, testing accuracy 0.2477 , training accuracy 0.3000 , learning rate 0.000147\n",
      "iterator 31000, testing accuracy 0.2432 , training accuracy 0.2000 , learning rate 0.000147\n",
      "iterator 32000, testing accuracy 0.2489 , training accuracy 0.3000 , learning rate 0.000147\n",
      "iterator 33000, testing accuracy 0.2329 , training accuracy 0.4000 , learning rate 0.000147\n",
      "iterator 34000, testing accuracy 0.2219 , training accuracy 0.2500 , learning rate 0.000147\n",
      "iterator 35000, testing accuracy 0.2375 , training accuracy 0.3500 , learning rate 0.000140\n",
      "iterator 36000, testing accuracy 0.2428 , training accuracy 0.2000 , learning rate 0.000140\n",
      "iterator 37000, testing accuracy 0.1866 , training accuracy 0.1500 , learning rate 0.000140\n",
      "iterator 38000, testing accuracy 0.2477 , training accuracy 0.2500 , learning rate 0.000140\n",
      "iterator 39000, testing accuracy 0.2128 , training accuracy 0.1500 , learning rate 0.000140\n",
      "iterator 40000, testing accuracy 0.2454 , training accuracy 0.3000 , learning rate 0.000133\n",
      "iterator 41000, testing accuracy 0.2219 , training accuracy 0.1500 , learning rate 0.000133\n",
      "iterator 42000, testing accuracy 0.2565 , training accuracy 0.2500 , learning rate 0.000133\n",
      "iterator 43000, testing accuracy 0.2276 , training accuracy 0.1500 , learning rate 0.000133\n",
      "iterator 44000, testing accuracy 0.2496 , training accuracy 0.3000 , learning rate 0.000133\n",
      "iterator 45000, testing accuracy 0.2390 , training accuracy 0.2000 , learning rate 0.000126\n",
      "iterator 46000, testing accuracy 0.2253 , training accuracy 0.0500 , learning rate 0.000126\n",
      "iterator 47000, testing accuracy 0.2462 , training accuracy 0.1500 , learning rate 0.000126\n",
      "iterator 48000, testing accuracy 0.2310 , training accuracy 0.0500 , learning rate 0.000126\n",
      "iterator 49000, testing accuracy 0.1976 , training accuracy 0.1000 , learning rate 0.000126\n",
      "iterator 50000, testing accuracy 0.2508 , training accuracy 0.2000 , learning rate 0.000120\n",
      "iterator 51000, testing accuracy 0.2584 , training accuracy 0.3500 , learning rate 0.000120\n",
      "iterator 52000, testing accuracy 0.2591 , training accuracy 0.2000 , learning rate 0.000120\n",
      "iterator 53000, testing accuracy 0.2435 , training accuracy 0.3000 , learning rate 0.000120\n",
      "iterator 54000, testing accuracy 0.1979 , training accuracy 0.3000 , learning rate 0.000120\n",
      "iterator 55000, testing accuracy 0.2584 , training accuracy 0.3000 , learning rate 0.000114\n",
      "iterator 56000, testing accuracy 0.2595 , training accuracy 0.3500 , learning rate 0.000114\n",
      "iterator 57000, testing accuracy 0.2451 , training accuracy 0.3000 , learning rate 0.000114\n",
      "iterator 58000, testing accuracy 0.2295 , training accuracy 0.4500 , learning rate 0.000114\n",
      "iterator 59000, testing accuracy 0.2686 , training accuracy 0.3000 , learning rate 0.000114\n",
      "iterator 60000, testing accuracy 0.2625 , training accuracy 0.3500 , learning rate 0.000108\n",
      "iterator 61000, testing accuracy 0.2519 , training accuracy 0.2000 , learning rate 0.000108\n",
      "iterator 62000, testing accuracy 0.2671 , training accuracy 0.1500 , learning rate 0.000108\n",
      "iterator 63000, testing accuracy 0.2378 , training accuracy 0.2000 , learning rate 0.000108\n",
      "iterator 64000, testing accuracy 0.2466 , training accuracy 0.3000 , learning rate 0.000108\n",
      "iterator 65000, testing accuracy 0.2511 , training accuracy 0.4500 , learning rate 0.000103\n",
      "iterator 66000, testing accuracy 0.2359 , training accuracy 0.2000 , learning rate 0.000103\n",
      "iterator 67000, testing accuracy 0.2143 , training accuracy 0.1000 , learning rate 0.000103\n",
      "iterator 68000, testing accuracy 0.2367 , training accuracy 0.1500 , learning rate 0.000103\n",
      "iterator 69000, testing accuracy 0.2625 , training accuracy 0.1500 , learning rate 0.000103\n",
      "iterator 70000, testing accuracy 0.2625 , training accuracy 0.1500 , learning rate 0.000098\n",
      "iterator 71000, testing accuracy 0.2728 , training accuracy 0.2500 , learning rate 0.000098\n",
      "iterator 72000, testing accuracy 0.2671 , training accuracy 0.2500 , learning rate 0.000098\n",
      "iterator 73000, testing accuracy 0.2337 , training accuracy 0.4000 , learning rate 0.000098\n",
      "iterator 74000, testing accuracy 0.2454 , training accuracy 0.2000 , learning rate 0.000098\n",
      "iterator 75000, testing accuracy 0.2451 , training accuracy 0.2000 , learning rate 0.000093\n",
      "iterator 76000, testing accuracy 0.2086 , training accuracy 0.0500 , learning rate 0.000093\n",
      "iterator 77000, testing accuracy 0.2481 , training accuracy 0.2000 , learning rate 0.000093\n",
      "iterator 78000, testing accuracy 0.2363 , training accuracy 0.1000 , learning rate 0.000093\n",
      "iterator 79000, testing accuracy 0.2576 , training accuracy 0.4500 , learning rate 0.000093\n",
      "iterator 80000, testing accuracy 0.2565 , training accuracy 0.2000 , learning rate 0.000088\n",
      "iterator 81000, testing accuracy 0.2663 , training accuracy 0.1500 , learning rate 0.000088\n",
      "iterator 82000, testing accuracy 0.2694 , training accuracy 0.2000 , learning rate 0.000088\n",
      "iterator 83000, testing accuracy 0.2443 , training accuracy 0.2500 , learning rate 0.000088\n",
      "iterator 84000, testing accuracy 0.2561 , training accuracy 0.2500 , learning rate 0.000088\n",
      "iterator 85000, testing accuracy 0.2606 , training accuracy 0.2500 , learning rate 0.000084\n",
      "iterator 86000, testing accuracy 0.2603 , training accuracy 0.3000 , learning rate 0.000084\n",
      "iterator 87000, testing accuracy 0.2416 , training accuracy 0.1000 , learning rate 0.000084\n",
      "iterator 88000, testing accuracy 0.2530 , training accuracy 0.2500 , learning rate 0.000084\n",
      "iterator 89000, testing accuracy 0.2599 , training accuracy 0.2000 , learning rate 0.000084\n",
      "iterator 90000, testing accuracy 0.2671 , training accuracy 0.2000 , learning rate 0.000079\n",
      "iterator 91000, testing accuracy 0.2819 , training accuracy 0.2500 , learning rate 0.000079\n",
      "iterator 92000, testing accuracy 0.2648 , training accuracy 0.2000 , learning rate 0.000079\n",
      "iterator 93000, testing accuracy 0.2295 , training accuracy 0.2500 , learning rate 0.000079\n",
      "iterator 94000, testing accuracy 0.2576 , training accuracy 0.1500 , learning rate 0.000079\n",
      "iterator 95000, testing accuracy 0.2565 , training accuracy 0.2000 , learning rate 0.000075\n",
      "iterator 96000, testing accuracy 0.2610 , training accuracy 0.2500 , learning rate 0.000075\n",
      "iterator 97000, testing accuracy 0.2329 , training accuracy 0.3000 , learning rate 0.000075\n",
      "iterator 98000, testing accuracy 0.2610 , training accuracy 0.2000 , learning rate 0.000075\n",
      "iterator 99000, testing accuracy 0.2553 , training accuracy 0.3000 , learning rate 0.000075\n",
      "iterator 100000, testing accuracy 0.2663 , training accuracy 0.2500 , learning rate 0.000072\n",
      "iterator 101000, testing accuracy 0.2644 , training accuracy 0.1500 , learning rate 0.000072\n",
      "iterator 102000, testing accuracy 0.2625 , training accuracy 0.4000 , learning rate 0.000072\n",
      "iterator 103000, testing accuracy 0.2413 , training accuracy 0.4500 , learning rate 0.000072\n",
      "iterator 104000, testing accuracy 0.2622 , training accuracy 0.3500 , learning rate 0.000072\n",
      "iterator 105000, testing accuracy 0.2500 , training accuracy 0.2500 , learning rate 0.000068\n",
      "iterator 106000, testing accuracy 0.2447 , training accuracy 0.4500 , learning rate 0.000068\n",
      "iterator 107000, testing accuracy 0.2511 , training accuracy 0.2000 , learning rate 0.000068\n",
      "iterator 108000, testing accuracy 0.2717 , training accuracy 0.3500 , learning rate 0.000068\n",
      "iterator 109000, testing accuracy 0.2781 , training accuracy 0.2000 , learning rate 0.000068\n",
      "iterator 110000, testing accuracy 0.2793 , training accuracy 0.3000 , learning rate 0.000065\n",
      "iterator 111000, testing accuracy 0.2675 , training accuracy 0.3000 , learning rate 0.000065\n",
      "iterator 112000, testing accuracy 0.2553 , training accuracy 0.2500 , learning rate 0.000065\n",
      "iterator 113000, testing accuracy 0.2595 , training accuracy 0.2500 , learning rate 0.000065\n",
      "iterator 114000, testing accuracy 0.2451 , training accuracy 0.2000 , learning rate 0.000065\n",
      "iterator 115000, testing accuracy 0.2622 , training accuracy 0.1500 , learning rate 0.000061\n",
      "iterator 116000, testing accuracy 0.2466 , training accuracy 0.2000 , learning rate 0.000061\n",
      "iterator 117000, testing accuracy 0.2542 , training accuracy 0.2000 , learning rate 0.000061\n",
      "iterator 118000, testing accuracy 0.2473 , training accuracy 0.4500 , learning rate 0.000061\n",
      "iterator 119000, testing accuracy 0.2766 , training accuracy 0.4000 , learning rate 0.000061\n",
      "iterator 120000, testing accuracy 0.2553 , training accuracy 0.3000 , learning rate 0.000058\n",
      "iterator 121000, testing accuracy 0.2709 , training accuracy 0.2000 , learning rate 0.000058\n",
      "iterator 122000, testing accuracy 0.2519 , training accuracy 0.3000 , learning rate 0.000058\n",
      "iterator 123000, testing accuracy 0.2614 , training accuracy 0.2500 , learning rate 0.000058\n",
      "iterator 124000, testing accuracy 0.2561 , training accuracy 0.2000 , learning rate 0.000058\n",
      "iterator 125000, testing accuracy 0.2679 , training accuracy 0.4000 , learning rate 0.000055\n",
      "iterator 126000, testing accuracy 0.2603 , training accuracy 0.1000 , learning rate 0.000055\n",
      "iterator 127000, testing accuracy 0.2648 , training accuracy 0.1500 , learning rate 0.000055\n",
      "iterator 128000, testing accuracy 0.2584 , training accuracy 0.5500 , learning rate 0.000055\n",
      "iterator 129000, testing accuracy 0.2439 , training accuracy 0.4500 , learning rate 0.000055\n",
      "iterator 130000, testing accuracy 0.2838 , training accuracy 0.1500 , learning rate 0.000053\n",
      "iterator 131000, testing accuracy 0.2618 , training accuracy 0.1500 , learning rate 0.000053\n",
      "iterator 132000, testing accuracy 0.2405 , training accuracy 0.1500 , learning rate 0.000053\n",
      "iterator 133000, testing accuracy 0.2633 , training accuracy 0.4500 , learning rate 0.000053\n",
      "iterator 134000, testing accuracy 0.2511 , training accuracy 0.1500 , learning rate 0.000053\n",
      "iterator 135000, testing accuracy 0.2447 , training accuracy 0.3000 , learning rate 0.000050\n",
      "iterator 136000, testing accuracy 0.2648 , training accuracy 0.3000 , learning rate 0.000050\n",
      "iterator 137000, testing accuracy 0.2717 , training accuracy 0.1500 , learning rate 0.000050\n",
      "iterator 138000, testing accuracy 0.2443 , training accuracy 0.4500 , learning rate 0.000050\n",
      "iterator 139000, testing accuracy 0.2694 , training accuracy 0.4500 , learning rate 0.000050\n",
      "iterator 140000, testing accuracy 0.2633 , training accuracy 0.3000 , learning rate 0.000048\n",
      "iterator 141000, testing accuracy 0.2743 , training accuracy 0.0500 , learning rate 0.000048\n",
      "iterator 142000, testing accuracy 0.2344 , training accuracy 0.5000 , learning rate 0.000048\n",
      "iterator 143000, testing accuracy 0.2656 , training accuracy 0.4500 , learning rate 0.000048\n",
      "iterator 144000, testing accuracy 0.2481 , training accuracy 0.4000 , learning rate 0.000048\n",
      "iterator 145000, testing accuracy 0.2606 , training accuracy 0.3500 , learning rate 0.000045\n",
      "iterator 146000, testing accuracy 0.2675 , training accuracy 0.2500 , learning rate 0.000045\n",
      "iterator 147000, testing accuracy 0.2610 , training accuracy 0.3000 , learning rate 0.000045\n",
      "iterator 148000, testing accuracy 0.2846 , training accuracy 0.3000 , learning rate 0.000045\n",
      "iterator 149000, testing accuracy 0.2739 , training accuracy 0.3500 , learning rate 0.000045\n",
      "iterator 150000, testing accuracy 0.2812 , training accuracy 0.4000 , learning rate 0.000043\n",
      "iterator 151000, testing accuracy 0.2622 , training accuracy 0.3000 , learning rate 0.000043\n",
      "iterator 152000, testing accuracy 0.2751 , training accuracy 0.3000 , learning rate 0.000043\n",
      "iterator 153000, testing accuracy 0.2496 , training accuracy 0.3000 , learning rate 0.000043\n",
      "iterator 154000, testing accuracy 0.2755 , training accuracy 0.2500 , learning rate 0.000043\n",
      "iterator 155000, testing accuracy 0.2462 , training accuracy 0.2500 , learning rate 0.000041\n",
      "iterator 156000, testing accuracy 0.2732 , training accuracy 0.1500 , learning rate 0.000041\n",
      "iterator 157000, testing accuracy 0.2557 , training accuracy 0.3000 , learning rate 0.000041\n",
      "iterator 158000, testing accuracy 0.2834 , training accuracy 0.3000 , learning rate 0.000041\n",
      "iterator 159000, testing accuracy 0.2656 , training accuracy 0.3500 , learning rate 0.000041\n",
      "iterator 160000, testing accuracy 0.2857 , training accuracy 0.2500 , learning rate 0.000039\n",
      "iterator 161000, testing accuracy 0.2656 , training accuracy 0.1000 , learning rate 0.000039\n",
      "iterator 162000, testing accuracy 0.2743 , training accuracy 0.2500 , learning rate 0.000039\n",
      "iterator 163000, testing accuracy 0.2663 , training accuracy 0.3500 , learning rate 0.000039\n",
      "iterator 164000, testing accuracy 0.2663 , training accuracy 0.1500 , learning rate 0.000039\n",
      "iterator 165000, testing accuracy 0.2663 , training accuracy 0.3000 , learning rate 0.000037\n",
      "iterator 166000, testing accuracy 0.2774 , training accuracy 0.3000 , learning rate 0.000037\n",
      "iterator 167000, testing accuracy 0.2572 , training accuracy 0.2000 , learning rate 0.000037\n",
      "iterator 168000, testing accuracy 0.2466 , training accuracy 0.3500 , learning rate 0.000037\n",
      "iterator 169000, testing accuracy 0.2850 , training accuracy 0.4000 , learning rate 0.000037\n",
      "iterator 170000, testing accuracy 0.2587 , training accuracy 0.2000 , learning rate 0.000035\n",
      "iterator 171000, testing accuracy 0.2622 , training accuracy 0.1500 , learning rate 0.000035\n",
      "iterator 172000, testing accuracy 0.2614 , training accuracy 0.4000 , learning rate 0.000035\n",
      "iterator 173000, testing accuracy 0.2637 , training accuracy 0.1000 , learning rate 0.000035\n",
      "iterator 174000, testing accuracy 0.2447 , training accuracy 0.2000 , learning rate 0.000035\n",
      "iterator 175000, testing accuracy 0.2747 , training accuracy 0.3000 , learning rate 0.000033\n",
      "iterator 176000, testing accuracy 0.2682 , training accuracy 0.4000 , learning rate 0.000033\n",
      "iterator 177000, testing accuracy 0.2470 , training accuracy 0.0500 , learning rate 0.000033\n",
      "iterator 178000, testing accuracy 0.2648 , training accuracy 0.3000 , learning rate 0.000033\n",
      "iterator 179000, testing accuracy 0.2724 , training accuracy 0.2000 , learning rate 0.000033\n",
      "iterator 180000, testing accuracy 0.2610 , training accuracy 0.3500 , learning rate 0.000032\n",
      "iterator 181000, testing accuracy 0.2477 , training accuracy 0.1500 , learning rate 0.000032\n",
      "iterator 182000, testing accuracy 0.2542 , training accuracy 0.3000 , learning rate 0.000032\n",
      "iterator 183000, testing accuracy 0.2432 , training accuracy 0.3500 , learning rate 0.000032\n",
      "iterator 184000, testing accuracy 0.2546 , training accuracy 0.3000 , learning rate 0.000032\n",
      "iterator 185000, testing accuracy 0.2652 , training accuracy 0.2500 , learning rate 0.000030\n",
      "iterator 186000, testing accuracy 0.2644 , training accuracy 0.1000 , learning rate 0.000030\n",
      "iterator 187000, testing accuracy 0.2853 , training accuracy 0.3000 , learning rate 0.000030\n",
      "iterator 188000, testing accuracy 0.2732 , training accuracy 0.3000 , learning rate 0.000030\n",
      "iterator 189000, testing accuracy 0.2770 , training accuracy 0.3500 , learning rate 0.000030\n",
      "iterator 190000, testing accuracy 0.2622 , training accuracy 0.3000 , learning rate 0.000028\n",
      "iterator 191000, testing accuracy 0.2766 , training accuracy 0.2500 , learning rate 0.000028\n",
      "iterator 192000, testing accuracy 0.2568 , training accuracy 0.1000 , learning rate 0.000028\n",
      "iterator 193000, testing accuracy 0.2793 , training accuracy 0.1500 , learning rate 0.000028\n",
      "iterator 194000, testing accuracy 0.2481 , training accuracy 0.2500 , learning rate 0.000028\n",
      "iterator 195000, testing accuracy 0.2785 , training accuracy 0.4500 , learning rate 0.000027\n",
      "iterator 196000, testing accuracy 0.2656 , training accuracy 0.1500 , learning rate 0.000027\n",
      "iterator 197000, testing accuracy 0.2827 , training accuracy 0.0500 , learning rate 0.000027\n",
      "iterator 198000, testing accuracy 0.2644 , training accuracy 0.2500 , learning rate 0.000027\n",
      "iterator 199000, testing accuracy 0.2701 , training accuracy 0.0500 , learning rate 0.000027\n",
      "iterator 200000, testing accuracy 0.2561 , training accuracy 0.4000 , learning rate 0.000026\n",
      "iterator 201000, testing accuracy 0.2846 , training accuracy 0.2500 , learning rate 0.000026\n",
      "iterator 202000, testing accuracy 0.2713 , training accuracy 0.3500 , learning rate 0.000026\n",
      "iterator 203000, testing accuracy 0.2652 , training accuracy 0.2500 , learning rate 0.000026\n",
      "iterator 204000, testing accuracy 0.2603 , training accuracy 0.1500 , learning rate 0.000026\n",
      "iterator 205000, testing accuracy 0.2690 , training accuracy 0.2000 , learning rate 0.000024\n",
      "iterator 206000, testing accuracy 0.2454 , training accuracy 0.3500 , learning rate 0.000024\n",
      "iterator 207000, testing accuracy 0.2572 , training accuracy 0.1500 , learning rate 0.000024\n",
      "iterator 208000, testing accuracy 0.2774 , training accuracy 0.3000 , learning rate 0.000024\n",
      "iterator 209000, testing accuracy 0.2591 , training accuracy 0.2500 , learning rate 0.000024\n",
      "iterator 210000, testing accuracy 0.2766 , training accuracy 0.5000 , learning rate 0.000023\n",
      "iterator 211000, testing accuracy 0.2603 , training accuracy 0.2000 , learning rate 0.000023\n",
      "iterator 212000, testing accuracy 0.2595 , training accuracy 0.1000 , learning rate 0.000023\n",
      "iterator 213000, testing accuracy 0.2580 , training accuracy 0.4500 , learning rate 0.000023\n",
      "iterator 214000, testing accuracy 0.2717 , training accuracy 0.4500 , learning rate 0.000023\n",
      "iterator 215000, testing accuracy 0.2648 , training accuracy 0.2500 , learning rate 0.000022\n",
      "iterator 216000, testing accuracy 0.2599 , training accuracy 0.3000 , learning rate 0.000022\n",
      "iterator 217000, testing accuracy 0.2671 , training accuracy 0.3000 , learning rate 0.000022\n",
      "iterator 218000, testing accuracy 0.2705 , training accuracy 0.1500 , learning rate 0.000022\n",
      "iterator 219000, testing accuracy 0.2606 , training accuracy 0.2500 , learning rate 0.000022\n",
      "iterator 220000, testing accuracy 0.2527 , training accuracy 0.3500 , learning rate 0.000021\n",
      "iterator 221000, testing accuracy 0.2584 , training accuracy 0.3500 , learning rate 0.000021\n",
      "iterator 222000, testing accuracy 0.2519 , training accuracy 0.3000 , learning rate 0.000021\n",
      "iterator 223000, testing accuracy 0.2538 , training accuracy 0.2000 , learning rate 0.000021\n",
      "iterator 224000, testing accuracy 0.2663 , training accuracy 0.3000 , learning rate 0.000021\n",
      "iterator 225000, testing accuracy 0.2698 , training accuracy 0.3000 , learning rate 0.000020\n",
      "iterator 226000, testing accuracy 0.2804 , training accuracy 0.2500 , learning rate 0.000020\n",
      "iterator 227000, testing accuracy 0.2633 , training accuracy 0.3000 , learning rate 0.000020\n",
      "iterator 228000, testing accuracy 0.2675 , training accuracy 0.1500 , learning rate 0.000020\n",
      "iterator 229000, testing accuracy 0.2690 , training accuracy 0.1000 , learning rate 0.000020\n",
      "iterator 230000, testing accuracy 0.2743 , training accuracy 0.3500 , learning rate 0.000019\n",
      "iterator 231000, testing accuracy 0.2561 , training accuracy 0.2000 , learning rate 0.000019\n",
      "iterator 232000, testing accuracy 0.2762 , training accuracy 0.4000 , learning rate 0.000019\n",
      "iterator 233000, testing accuracy 0.2568 , training accuracy 0.3500 , learning rate 0.000019\n",
      "iterator 234000, testing accuracy 0.2789 , training accuracy 0.3500 , learning rate 0.000019\n",
      "iterator 235000, testing accuracy 0.2618 , training accuracy 0.2000 , learning rate 0.000018\n",
      "iterator 236000, testing accuracy 0.2804 , training accuracy 0.2500 , learning rate 0.000018\n",
      "iterator 237000, testing accuracy 0.2603 , training accuracy 0.4500 , learning rate 0.000018\n",
      "iterator 238000, testing accuracy 0.2622 , training accuracy 0.2000 , learning rate 0.000018\n",
      "iterator 239000, testing accuracy 0.2565 , training accuracy 0.2000 , learning rate 0.000018\n",
      "iterator 240000, testing accuracy 0.2762 , training accuracy 0.2000 , learning rate 0.000017\n",
      "iterator 241000, testing accuracy 0.2675 , training accuracy 0.2000 , learning rate 0.000017\n",
      "iterator 242000, testing accuracy 0.2576 , training accuracy 0.1000 , learning rate 0.000017\n",
      "iterator 243000, testing accuracy 0.2698 , training accuracy 0.1000 , learning rate 0.000017\n",
      "iterator 244000, testing accuracy 0.2732 , training accuracy 0.4000 , learning rate 0.000017\n",
      "iterator 245000, testing accuracy 0.2462 , training accuracy 0.4000 , learning rate 0.000016\n",
      "iterator 246000, testing accuracy 0.2622 , training accuracy 0.2500 , learning rate 0.000016\n",
      "iterator 247000, testing accuracy 0.2751 , training accuracy 0.1000 , learning rate 0.000016\n",
      "iterator 248000, testing accuracy 0.2587 , training accuracy 0.2500 , learning rate 0.000016\n",
      "iterator 249000, testing accuracy 0.2785 , training accuracy 0.2000 , learning rate 0.000016\n",
      "iterator 250000, testing accuracy 0.2633 , training accuracy 0.1500 , learning rate 0.000015\n",
      "iterator 251000, testing accuracy 0.2713 , training accuracy 0.3000 , learning rate 0.000015\n",
      "iterator 252000, testing accuracy 0.2572 , training accuracy 0.2000 , learning rate 0.000015\n",
      "iterator 253000, testing accuracy 0.2808 , training accuracy 0.2500 , learning rate 0.000015\n",
      "iterator 254000, testing accuracy 0.2777 , training accuracy 0.3000 , learning rate 0.000015\n",
      "iterator 255000, testing accuracy 0.2561 , training accuracy 0.4500 , learning rate 0.000015\n",
      "iterator 256000, testing accuracy 0.2595 , training accuracy 0.2000 , learning rate 0.000015\n",
      "iterator 257000, testing accuracy 0.2679 , training accuracy 0.1500 , learning rate 0.000015\n",
      "iterator 258000, testing accuracy 0.2591 , training accuracy 0.4000 , learning rate 0.000015\n",
      "iterator 259000, testing accuracy 0.2671 , training accuracy 0.3500 , learning rate 0.000015\n",
      "iterator 260000, testing accuracy 0.2603 , training accuracy 0.2000 , learning rate 0.000014\n",
      "iterator 261000, testing accuracy 0.2644 , training accuracy 0.1000 , learning rate 0.000014\n",
      "iterator 262000, testing accuracy 0.2523 , training accuracy 0.2000 , learning rate 0.000014\n",
      "iterator 263000, testing accuracy 0.2823 , training accuracy 0.3500 , learning rate 0.000014\n"
     ]
    }
   ],
   "source": [
    "tf.set_random_seed(1)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "     \n",
    "    batches = batch_iter(list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "    i = 0\n",
    "    for batch in batches:\n",
    "        i = i + 1\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        sess.run(train_step, feed_dict={x:x_batch, y:y_batch})\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "\n",
    "            acc_train = sess.run(accuracy, feed_dict = {x:x_batch, y:y_batch})\n",
    "            acc_test = sess.run(accuracy, feed_dict = {x:x_test, y:y_test})\n",
    "            ts = sess.run(global_step)\n",
    "            lr = sess.run(learning_rate)\n",
    "            result_list[0].append(i)\n",
    "            result_list[1].append(acc_train)\n",
    "            result_list[2].append(acc_test)\n",
    "            print(\"iterator \" + str(i) + \", testing accuracy %.4f\" % acc_test , \", training accuracy %.4f\" % acc_train, \n",
    "                  \", learning rate %f\" % lr )\n",
    "    save_path = saver.save(sess, \"/home/leo/Desktop/cmpt318_project/model/tf_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(result_list[0], result_list[1], 'g')\n",
    "plt.plot(result_list[0], result_list[2], 'b')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train accuracy', 'Test accuracy'])\n",
    "plt.savefig('tf_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
